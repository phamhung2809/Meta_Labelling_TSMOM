{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Thêm thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Các thư viện cần thiết, trong đó có `yfinance` để lấy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn import tree\n",
    "import xgboost as xgb\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from hypopt import GridSearch\n",
    "import keras_tuner as kt\n",
    "\n",
    "import yfinance as yf\n",
    "\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Lấy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hàm lấy data, là cổ phiếu của 50 công ty trên sàn `EURO_STOXX_50` \\\n",
    "Kết quả trả về là 1 dataframe có dạng m dòng, 50 cột với m là time range "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EU_Stock_data(start_time,end_time, time_range = 'max'):\n",
    "    \"\"\"Lấy dữ liệu giá Close của 50 công ty trên sàn Euro_STOXX 50 vào thời gian cho trước\"\"\"\n",
    "\n",
    "    stock_list = pd.read_html( 'https://en.wikipedia.org/wiki/EURO_STOXX_50')[4]['Ticker'].to_list()\n",
    "\n",
    "    futures = pd.DataFrame()  \n",
    "\n",
    "    # xét từng mã\n",
    "    for symbol in stock_list:\n",
    "        try:\n",
    "            df = yf.Ticker(symbol).history(period = time_range, start = start_time, end = end_time)\n",
    "            df = pd.DataFrame(df['Close'])\n",
    "            df.columns = [symbol]\n",
    "            df.index = df.index.date\n",
    "            futures = pd.concat([futures,df],axis = 1, join = 'outer').sort_index()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    futures['Date'] = pd.to_datetime(futures.index, format='%Y-%m-%d')\n",
    "    futures.set_index('Date', inplace=True)\n",
    "\n",
    "    return futures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Classic TSMOM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm thực hiện tính toán để lấy về giá trị volatility (biến động) của mỗi ngày"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Volatility_scale(data, ignore_na=False, adjust = True, com = 60, min_periods=0):\n",
    "    \"\"\"Scale data using ex ante volatility\"\"\"\n",
    "\n",
    "    # Lưu trữ index, tức thời gian \n",
    "    std_index = data.index\n",
    "\n",
    "    # chứa kết quả\n",
    "    daily_index = pd.DataFrame(index=std_index)\n",
    "\n",
    "    # xét từng cổ phiếu\n",
    "    for oo in data.columns:\n",
    "        returns = data[oo]  # Lấy ra các return\n",
    "        returns.dropna(inplace=True)  # xử lý null bằng zero\n",
    "\n",
    "        returns = returns.rolling(2).apply(lambda x: x.iloc[1] / x.iloc[0] - 1)\n",
    "        returns.iloc[0] =  0\n",
    "\n",
    "        # Tính cumulative (cum) return , nhưng ko có thành phần - 1\n",
    "        ret_index = (1 + returns).cumprod()\n",
    "\n",
    "        # Tính daily volatility (vol)\n",
    "        day_vol = returns.ewm(ignore_na=ignore_na,\n",
    "                              adjust=adjust,\n",
    "                              com=com,\n",
    "                              min_periods=min_periods).std(bias=False)\n",
    "        \n",
    "        vol = day_vol * np.sqrt(252)  # scale lại theo 252 ngày active trading\n",
    "\n",
    "        # Join cum return và vol\n",
    "        ret_index = pd.concat([ret_index, vol], axis=1)\n",
    "        ret_index.columns = [oo, oo + '_Vol']  # Đặt tên cột cum return là tên cổ phiếu, bên cạnh là vol \n",
    "\n",
    "        # Join \n",
    "        daily_index = pd.concat([daily_index, ret_index], join = 'outer' ,axis=1)\n",
    "\n",
    "    return daily_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm implement chiến lược TSMOM, với logic cụ thể như sau:\n",
    "Tại ngày t ta so  với ngày t - k về trước, cụ thể ta có thể lấy giá close,\n",
    " hoặc cumulative return (nhưng không có thành phần - 1, tức $\\text{cum return}_t = \\prod_{i = 0}^{t} (1 + r_i)$), \n",
    "ở đây xét `cum_return_t` với của k ngày trước\n",
    "`cum_return_{t-k}`\n",
    "  - Giả sử `cum_return_t` > `cum_return_{t-k}` tức `sign(cum_return_t - cum_return_{t - k}) = 1` (hàm dấu trả về 1 nếu input > 0)  thì ta có signal = 1, tức đó là tín hiệu để vào lệnh long vào ngày mai \n",
    "(ngày t + 1), ngược lại thì signal = -1, là tín hiệu vào short\n",
    "  -  Sau đó hold trong h -1 ngày tiếp theo (ngày t + 1 vào long đã bắt đầu tính là hold). \n",
    "  - Trong các ngày này (tức t + i với i từ 1 đến h), đều có sinh ra Profit and Loss (PnL)  tính theo công thức:\\\n",
    " ` 0.4/ vol_t * return_{t, t + i}` với `return_{t, t + i}` là return trong giai đoạn t đến t + i, tính tùy vào trường hợp long hay short:\n",
    "      - nếu long, `return_{t, t + i}` = 1 - `cum_return_t / cum_return_{t + i}`\n",
    "      - nếu short, `return_{t, t + i}` =  1 - `cum_return_{t + i} / cum_return_t` \n",
    "      \n",
    "    và Leverage, là ` target_vol / vol_t`   (target_vol đang để là 0.4)\n",
    " \n",
    " Tóm lại, Các kết quả trả về lần lượt là: \n",
    "- profit and loss `pnl` \n",
    "- `leverage`\n",
    "- `signal`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classic_TSMOM(data, k, h, tolerance = 0,ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    \n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    company = data.columns\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "        df['rolling returns'] = df[oo].pct_change(k) # so sánh thay đổi ở ngày t với k ngày trước đó (tức t - k)\n",
    "        df['signal'] = 0.\n",
    "        for x, v in enumerate(df['rolling returns']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if df['rolling returns'].iloc[x-1] < tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return < 0, short rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = -1\n",
    "                \n",
    "                elif df['rolling returns'].iloc[x-1] > tolerance:\n",
    "                    for h_period in range(0,h):\n",
    "                        # rolling return > 0, long rồi giữ trong h ngày, tính pnl, leverage///\n",
    "                        df['signal'].iloc[x + h_period] = 1\n",
    "\n",
    "            except:pass\n",
    "            \n",
    "\n",
    "            # Đặt flag holding là h - 1, để qua vòng for mới bỏ qua ngày hold, tránh bị tính lặp lại\n",
    "            if df['rolling returns'].iloc[x-1] != tolerance: flag_h = h - 1\n",
    "\n",
    "        signal = pd.concat([signal, df['signal']], join = 'outer', axis=1)\n",
    "\n",
    "    signal.columns = data.columns\n",
    "    \n",
    "    return signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MACD(data,period_fast,period_slow):\n",
    "    EMA_fast = pd.Series(\n",
    "        data.ewm(ignore_na=False, span=period_fast, adjust=True).mean()\n",
    "    )\n",
    "    EMA_Slow = pd.Series(\n",
    "        data.ewm(ignore_na=False, span=period_slow, adjust=True).mean()\n",
    "    )\n",
    "    return EMA_fast - EMA_Slow\n",
    "\n",
    "def MACD_normalized(data, period_fast, period_slow):\n",
    "    macd = MACD(data, period_fast, period_slow)\n",
    "    ewm_std_63 = data.ewm(span=63).std()\n",
    "    q = macd / ewm_std_63\n",
    "    z = q / q.ewm(span=252).std()\n",
    "    return z\n",
    "\n",
    "def calculate_rsi(data, period  = 14):\n",
    "    # daily changes\n",
    "    delta = data.diff()\n",
    "\n",
    "    gain = (delta.where(delta > 0, 0))\n",
    "    loss = (-delta.where(delta < 0, 0))\n",
    "\n",
    "    # average gain and loss over the period\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_features_single_asset(df,k,h,test = False):\n",
    "    df = df.dropna(how='any',axis=0)\n",
    "    if df.shape[0] < 64: \n",
    "        return None\n",
    "    df[\"Return Daily\"] = df[\"Close\"].rolling(2).apply(lambda x: x.iloc[1] / x.iloc[0] - 1)\n",
    "    df[\"Return Daily\"].iloc[0] =  0\n",
    "    df[\"Volatility_Scale\"] = Volatility_scale(pd.DataFrame(df[\"Close\"]))[\"Close_Vol\"]\n",
    "    df['Cummulative Return'] = (1+ df['Return Daily']).cumprod(axis = 0)\n",
    "    df['Mean H Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].mean()).shift(-h)\n",
    "    df['Next H Return'] = df['Cummulative Return'].pct_change(h).shift(-h)\n",
    "    df['Square Sum Return'] = df[\"Return Daily\"].rolling(h+1).apply(lambda x: x.iloc[range(1,h+1)].pow(2).sum()).shift(-h)\n",
    "    df[\"Next H Vol\"] = df[\"Volatility_Scale\"].shift(-h)\n",
    "    df['Next H PnL'] = df['Next H Return'] / df[\"Next H Vol\"] \n",
    "\n",
    "    for temp in [k,1]:\n",
    "        df[\"Past \" + str(temp) + \" Day\" ] = df['Close'].pct_change(temp) / (df[\"Volatility_Scale\"] * np.sqrt(252))\n",
    "    \n",
    "    # df[\"MACD_8_24\"] = MACD_normalized(df[\"Close\"],8,24)  # paper .?\n",
    "    df[\"MACD_19_39\"] = MACD_normalized(df[\"Close\"],19,39) # for longer trend\n",
    "    df[\"MACD_5_13\"] = MACD_normalized(df[\"Close\"], 5, 13) ## for fast trend\n",
    "    # df[\"MACD_12_26\"] = MACD_normalized(df[\"Close\"], 12, 26) ## most common\n",
    "\n",
    "    # RSI for overbought/oversold\n",
    "    df['RSI_5'] = calculate_rsi(df['Close'], period=5)\n",
    "    # df['RSI_10'] = calculate_rsi(df['Close'], period=10)\n",
    "    # df['RSI_14'] = calculate_rsi(df['Close'], period = 14)\n",
    "    ## price sma\n",
    "    df['Price_SMA_5'] = df['Close'] / df['Close'].rolling(5).mean() - 1\n",
    "\n",
    "\n",
    "    df['Signal'] = [1 if x > 0 else 0 for x in df['Next H Return']]\n",
    "    \n",
    "    df = df.dropna(how='any',axis=0)\n",
    "    \n",
    "    temp = pd.DataFrame(columns= df.columns)\n",
    "    n = 0\n",
    "    while True:\n",
    "        try:\n",
    "            temp = pd.concat([temp,df.iloc[[n*h],:]], axis = 0)\n",
    "            n = n+1\n",
    "        except: break\n",
    "    \n",
    "    try:\n",
    "        df = temp[:-2]\n",
    "    except:\n",
    "        df = None\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(data,k,h,supervised = False, binary = True):\n",
    "    company = data.columns\n",
    "    features = []\n",
    "    for i in [k,1]:\n",
    "        features.append(\"Past \" + str(i) + \" Day\")\n",
    "    \n",
    "    features.append(\"MACD_19_39\")\n",
    "    features.append(\"MACD_5_13\")\n",
    "    features.append('RSI_5')\n",
    "    # features.append('RSI_10')\n",
    "    features.append('Price_SMA_5')\n",
    "    \n",
    "    X_train = pd.DataFrame(columns=features)\n",
    "    if supervised == False:\n",
    "        y_train = pd.DataFrame(columns=[\"Mean H Return\",\"Square Sum Return\",\"Volatility_Scale\"])\n",
    "    elif supervised == True:\n",
    "        if binary == True:\n",
    "            y_train = pd.DataFrame(columns=[\"Signal\"])\n",
    "        else:\n",
    "            y_train = pd.DataFrame(columns=[\"Next H PnL\"])\n",
    "            \n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        df.columns = [\"Close\"]\n",
    "\n",
    "        df = construct_features_single_asset(df,k,h)\n",
    "\n",
    "        if df is None:\n",
    "            continue\n",
    "        \n",
    "        X_train = pd.concat([X_train,df[features]],axis = 0)\n",
    "        if supervised == False:\n",
    "            y_train = pd.concat([y_train,df[[\"Mean H Return\",\"Square Sum Return\",\"Volatility_Scale\",\"Next H Return\"]]],axis = 0)\n",
    "        elif supervised == True:\n",
    "            if binary == True:\n",
    "                y_train = pd.concat([y_train,df[[\"Signal\"]]],axis = 0)\n",
    "            else:\n",
    "                y_train = pd.concat([y_train,df[[\"Next H PnL\"]]],axis = 0)\n",
    "            \n",
    "    return [X_train,y_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_decision_tree(X_train,y_train,X_val,y_val,k,h):\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 10, 15],\n",
    "        'min_samples_split': [2, 5, 10, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "\n",
    "    \n",
    "    model = GridSearch(model = tree.DecisionTreeClassifier(random_state=42), param_grid = param_grid,parallelize=False)\n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_xgboost(X_train,y_train,X_val,y_val,k,h):\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.3],\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0]\n",
    "    }\n",
    "\n",
    "    model = GridSearch(model = xgb.XGBClassifier(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='logloss'\n",
    "    ), param_grid = param_grid,parallelize=False)\n",
    "    \n",
    "    model.fit(X_train, y_train, X_val, y_val)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_supervised(kt.HyperModel):\n",
    "    def __init__(self, k,binary):\n",
    "        self.k = k\n",
    "        self.binary = binary\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(6,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 20, 40]),activation = hp.Choice('activation', ['relu'])),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.3, 0.5])),\n",
    "            Dense(1,activation = 'sigmoid' if self.binary else None),\n",
    "        ])\n",
    "\n",
    "        if self.binary == True:\n",
    "            loss = 'binary_crossentropy'\n",
    "        else: loss = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= loss,\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):        \n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_supervised(X_train,y_train,X_val,y_val,k,h,binary = True):\n",
    "    \n",
    "    tuner = kt.GridSearch(\n",
    "        MLP_supervised(k = k,binary = binary),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name= f\"tune_MLP_supervised_{'binary' if binary else 'reg'}\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = (\n",
    "        'Test/Data/checkpoint_mlp_sup_binary.model.keras' if binary\n",
    "        else 'Test/Data/checkpoint_mlp_sup_reg.model.keras'\n",
    "    )\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True, verbose = 1)    \n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_supervised(k = k,binary = binary)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_supervised(kt.HyperModel):\n",
    "    def __init__(self, k,binary):\n",
    "        self.k = k\n",
    "        self.binary = binary\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (6,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-4, 1e-3, 1e-2, 0.1,])),activation= 'sigmoid' if self.binary else None)\n",
    "        ])\n",
    "        \n",
    "        if self.binary == True:\n",
    "            loss = 'binary_crossentropy'\n",
    "        else: loss = tf.keras.metrics.RootMeanSquaredError()\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= loss,\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "    \n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512,1024]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_supervised(X_train,y_train,X_val,y_val,k,h,binary = True):\n",
    "\n",
    "    X_val = np.array(X_val, dtype=np.float64)\n",
    "    y_val = np.array(y_val, dtype=np.float64)\n",
    "    tuner = kt.GridSearch(\n",
    "        Lasso_supervised(k = k,binary=binary),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name= f\"tune_Lasso_supervised_{'binary' if binary else 'reg'}\",\n",
    "\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = (\n",
    "        'Test/Data/checkpoint_lasso_sup_binary.model.keras' if binary\n",
    "        else 'Test/Data/checkpoint_lasso_sup_reg.model.keras'\n",
    "    )\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es],validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_supervised(k = k,binary=binary)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    \n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_loss(h,target_vol = 0.2):\n",
    "    def calculation(y_target_dummy, y_pred):\n",
    "\n",
    "        mean = K.reshape(y_target_dummy[:, 0], (-1, 1))\n",
    "        square_sum =  K.reshape(y_target_dummy[:, 1], (-1, 1))\n",
    "        volatility_scale = K.reshape(y_target_dummy[:, 2], (-1, 1))\n",
    "        next_h_return = K.reshape(y_target_dummy[:, 3], (-1, 1))\n",
    "\n",
    "        sum_pofolio = mean * h * y_pred * target_vol / volatility_scale\n",
    "        mean_pofolio = K.mean(mean * h * y_pred * target_vol / volatility_scale) / h\n",
    "\n",
    "        std_pofolio = tf.math.sqrt(K.mean(square_sum * y_pred **2  * (target_vol / volatility_scale)**2\n",
    "                                          - 2 * sum_pofolio * mean_pofolio \n",
    "                                          + (mean_pofolio ** 2) * h)/h)\n",
    "\n",
    "    \n",
    "        return  - (mean_pofolio / std_pofolio) *np.sqrt(252) + tf.math.sqrt(mean((next_h_return/volatility_scale - y_pred) ** 2))\n",
    "    \n",
    "    return calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dropout(0, input_shape=(6,)),\n",
    "            Dense(units=hp.Choice(f\"units\", [5, 20]),activation = hp.Choice('activation', ['tanh', 'relu'])),\n",
    "            Dropout(rate=hp.Choice(\"dropout\", [0.1, 0.3, 0.5])),\n",
    "            Dense(1,activation = 'tanh'),\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-4, 1e-3, 1e-2, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MLP_sharpeLoss(X_train,y_train,X_val,y_val,k,h):\n",
    "\n",
    "    tuner = kt.GridSearch(\n",
    "        MLP_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_MLP_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_MLP_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)    \n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = MLP_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback],validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso (Sharpe Loss optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lasso_SharpeLoss(kt.HyperModel):\n",
    "    def __init__(self, k,h):\n",
    "        self.k = k\n",
    "        self.h = h\n",
    "\n",
    "    def build(self,hp):\n",
    "        model = Sequential([\n",
    "            Dense(1, input_shape = (6,),kernel_regularizer = l1(hp.Choice(\"l1_weight\", [1e-3, 1e-2, 0.1,])),activation='sigmoid')\n",
    "        ])\n",
    "\n",
    "        model.compile(\n",
    "            optimizer=Adam(\n",
    "                learning_rate=hp.Choice(\"learning_rate\", [1e-3, 1e-1, 1.0]),\n",
    "                clipnorm = hp.Choice(\"max_grad_norm\", [1e-2, 0.1, 1.0, 10.0])\n",
    "            ),\n",
    "            loss= sharpe_loss(h = self.h)\n",
    "        )\n",
    "        return model\n",
    "    def fit(self, hp, model, *args, **kwargs):\n",
    "\n",
    "        return model.fit(\n",
    "            *args,\n",
    "            batch_size=hp.Choice(\"batch_size\", [256,512]),\n",
    "            **kwargs, epochs = 100, verbose=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_Lasso_sharpeLoss(X_train,y_train,X_val,y_val,k,h):\n",
    "\n",
    "    tuner = kt.GridSearch(\n",
    "        Lasso_SharpeLoss(k = k,h=h),\n",
    "        objective=\"val_loss\",\n",
    "        max_trials=50,\n",
    "        overwrite=True,\n",
    "        directory=\"tuning_dir\",\n",
    "        project_name=\"tune_Lasso_sharpeLoss\",\n",
    "    )\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, patience=25)\n",
    "\n",
    "    checkpoint_filepath = 'Test/Data/checkpoint_Lasso_sharpeLoss.model.keras'\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor = 'val_loss',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "    tuner.search(X_train, y_train, callbacks = [es], validation_data=(X_val, y_val))\n",
    "\n",
    "    hypermodel = Lasso_SharpeLoss(k = k,h= h)\n",
    "    best_hp = tuner.get_best_hyperparameters()[0]\n",
    "    model = hypermodel.build(best_hp)\n",
    "\n",
    "    history = hypermodel.fit(best_hp,model,X_train, y_train,callbacks = [model_checkpoint_callback], validation_data = (X_val, y_val))\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_TSMOM(data, model,k,h):\n",
    "\n",
    "    company = data.columns\n",
    "\n",
    "    signal = pd.DataFrame(index = data.index)\n",
    "\n",
    "    features = []\n",
    "    for i in [k,1]:\n",
    "        features.append(\"Past \" + str(i) + \" Day\")\n",
    "\n",
    "    features.append(\"MACD_19_39\")\n",
    "    features.append(\"MACD_5_13\")\n",
    "    features.append('RSI_5')\n",
    "    # features.append('RSI_10')\n",
    "    features.append('Price_SMA_5')\n",
    "\n",
    "    for oo in company:\n",
    "        df = data[[oo]].copy()\n",
    "        \n",
    "        df.columns = [\"Close\"]\n",
    "        df = construct_features_single_asset(df,k,h,test= True)\n",
    "\n",
    "        if df is None: continue\n",
    "        time_index = data[oo].dropna(how = 'any').index\n",
    "        company_signal = pd.DataFrame(index = time_index, columns = [oo])\n",
    "        \n",
    "        X_test = df[features]\n",
    "        if X_test.shape[0] == 0: continue\n",
    "\n",
    "        # Take signal\n",
    "        # try:\n",
    "        #     if model.loss ==  'binary_crossentropy':\n",
    "        #         X_test['prediction'] = np.sign(model.predict(X_test) - 0.5)\n",
    "        #     else:\n",
    "        #         X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "        # except:\n",
    "        #     X_test['prediction'] = np.sign(model.predict(X_test))\n",
    "        #     X_test['prediction'][X_test['prediction'] == 0] = -1\n",
    "\n",
    "        # Take Direct Output\n",
    "        try:\n",
    "            if model.loss ==  'binary_crossentropy':\n",
    "                X_test['prediction'] = np.sign(model.predict(X_test) - 0.5)\n",
    "                X_test['prediction'][X_test['prediction'] == 0] = -1\n",
    "            else:\n",
    "                X_test['prediction'] = model.predict(X_test)\n",
    "        except:\n",
    "            X_test['prediction'] = model.predict(X_test)\n",
    "        \n",
    "        for x,v in enumerate(X_test.index):\n",
    "            company_signal.loc[v,oo] = X_test.loc[v,'prediction']\n",
    "        \n",
    "        company_signal = company_signal.ffill()\n",
    "        company_signal = company_signal.fillna(0)\n",
    "\n",
    "        signal = pd.concat([signal,company_signal], axis = 1, join = 'outer')\n",
    "\n",
    "\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_history(history, model_name):\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    ax.plot(history.history['loss'])\n",
    "    ax.plot(history.history['val_loss'])\n",
    "    ax.set_title(str(model_name) + ' loss')\n",
    "    ax.set_ylabel('loss')\n",
    "    ax.set_xlabel('epoch')\n",
    "    ax.legend(['train', 'validation'], loc='upper left')\n",
    "    fig.savefig(str(model_name) + 'loss.png')\n",
    "    del fig\n",
    "    del ax\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(data,signal,k,h,  vol_flag = 1, target_vol = 0.2, ignore_na = False, adjust = True, com = 60, min_periods = 0):\n",
    "    \n",
    "    pnl = pd.DataFrame(index=data.index)\n",
    "    leverage = pd.DataFrame(index = data.index)\n",
    "\n",
    "    company = signal.columns\n",
    "\n",
    "    # gọi hàm Volatility scale\n",
    "    daily_index = Volatility_scale(data,ignore_na=ignore_na,\n",
    "                          adjust=adjust,\n",
    "                          com=com,   \n",
    "                          min_periods = min_periods)\n",
    "\n",
    "\n",
    "    # Volatility settings\n",
    "    vol_flag = vol_flag    # Set flag to 1 for vol targeting\n",
    "    if vol_flag == 1:\n",
    "        target_vol = target_vol \n",
    "    else:\n",
    "        target_vol = 'no target vol'\n",
    "    \n",
    "\n",
    "    for oo in company:\n",
    "        flag_h = 0\n",
    "        flag_k = k+1\n",
    "        df = pd.concat([daily_index[oo], daily_index[oo+\"_Vol\"]], axis=1)\n",
    "        df = df.dropna(axis = 0, how = 'all')\n",
    "\n",
    "        company_signal = signal[oo].dropna(axis = 0, how = 'all')\n",
    "        df['pnl'] = 0. \n",
    "        df['leverage'] = 0.\n",
    "        for x, v in enumerate(df['pnl']):\n",
    "            if flag_h != 0:\n",
    "                # Bỏ qua giai đoạn hold, tránh bị tính lặp lại\n",
    "                flag_h = flag_h - 1\n",
    "                continue\n",
    "            # Bỏ qua thời gian cty chưa được lên sàn (nêu có)\n",
    "            if df[oo].isnull().iloc[x] == False:\n",
    "                # bỏ qua k ngày đầu vì chưa đủ k lookback\n",
    "                if flag_k != 0:\n",
    "                    flag_k = flag_k - 1\n",
    "                    continue\n",
    "            else: continue\n",
    "            try:\n",
    "                if company_signal.iloc[x] == -1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period]) * \\\n",
    "                                target_vol / df[oo+\"_Vol\"].iloc[x -1] \n",
    "                            df['leverage'].iloc[x + h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (1 - df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period])\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "                elif company_signal.iloc[x] == 1:\n",
    "                    for h_period in range(0,h):\n",
    "                        if vol_flag == 1:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1) * \\\n",
    "                                    target_vol / df[oo+\"_Vol\"].iloc[x - 1]\n",
    "                            df['leverage'].iloc[x+h_period] = target_vol / df[oo+\"_Vol\"].iloc[x -1]\n",
    "                        else:\n",
    "                            df['pnl'].iloc[x + h_period] = (df[oo].iloc[x + h_period] / df[oo].iloc[x - 1 + h_period] - 1)\n",
    "                            df['leverage'].iloc[x+h_period] = 1\n",
    "            except:pass\n",
    "            \n",
    "            if signal[oo].iloc[x] == 1 or signal[oo].iloc[x] == -1 : flag_h = h - 1\n",
    "\n",
    "\n",
    "        leverage = pd.concat([leverage, df['leverage']], join = 'outer',axis = 1)\n",
    "        pnl = pd.concat([pnl, df['pnl']], join = 'outer',axis=1)\n",
    "\n",
    "    pnl.columns = signal.columns\n",
    "    leverage.columns = signal.columns\n",
    "\n",
    "    return [pnl,leverage]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuối cùng, ta lấy mean của 50 cổ phiếu để có `PnL` đại diện "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_daily_return(pnl):\n",
    "    \n",
    "    return pnl.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Example Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Lấy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Thời gian input theo dạng yyyy-mm-dd; với ví dụ ở dưới \n",
    "\n",
    "# start_time = '2004-12-31'\n",
    "# end_time = '2010-01-01' \n",
    "\n",
    "# df = EU_Stock_data(start_time,end_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Code demo Classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2009-12-31'\n",
    "end_time = '2024-12-22'\n",
    "\n",
    "daily_return = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "daily_index = Volatility_scale(daily_return)\n",
    "\n",
    "# print ra result là pnl, leverage, signal của hàm backtest_strategy(), với k = 3, h = 3, target volatility = 0.4\n",
    "LOOKBACK = 3\n",
    "HOLDING = 3\n",
    "TARGET_VOL = 0.4\n",
    "\n",
    "signal = classic_TSMOM(daily_return,LOOKBACK,HOLDING)\n",
    "[pnl,leverage] = backtest(daily_return,signal,LOOKBACK,HOLDING, target_vol= TARGET_VOL)\n",
    "\n",
    "print(f'pnl với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "pnl\n",
    "\n",
    "print(f'leverage với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "leverage\n",
    "\n",
    "print(f'signal với k = {LOOKBACK} , h = {HOLDING}, target volatility = {TARGET_VOL}:')\n",
    "signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Code so sánh các cặp k,h khi sử dụng classic TSMOM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2019-12-31'\n",
    "end_time = '2024-12-31'\n",
    "\n",
    "data = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "for k in range(1,11):\n",
    "    for h in range(1,11):\n",
    "        # print([k,h]) # Kiểm tra tiến độ\n",
    "        signal = classic_TSMOM(data,k,h)\n",
    "        result = backtest(data,signal,k,h)\n",
    "        # result[0].to_csv(\"pnl (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\") + str(h) + \").csv\")\n",
    "        # result[1].to_csv(\"leverage (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\")  + str(h) + \").csv\")\n",
    "        # result[2].to_csv(\"signal (k = \" + str(0 if k < 10 else \"\") + str(k) + \", h = \" + str(0 if h < 10 else \"\")  + str(h) + \").csv\")\n",
    "        temp = strategy_daily_return(result[0])\n",
    "        try:\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        except:\n",
    "            index = temp.index.to_list()\n",
    "            index.insert(0,'h')\n",
    "            index.insert(0,'k')\n",
    "            stats = pd.DataFrame(columns = index)\n",
    "            temp2 = temp.to_list()\n",
    "            temp2.insert(0,h)\n",
    "            temp2.insert(0,k)\n",
    "            stats.loc[len(stats.index)] = temp2\n",
    "        del result\n",
    "\n",
    "stats.to_csv(\"k_h_Comparing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4. Code demo thử model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train theo tỉ lệ Data (9 năm Train; 1 năm Validation và 5 năm Backtest từ 15 năm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2018-12-31'\n",
    "end_time = '2024-12-22'\n",
    "\n",
    "# start_time = '2004-12-31'\n",
    "# end_time = '2019-12-31'\n",
    "\n",
    "k = 5\n",
    "h = 10\n",
    "\n",
    "data = EU_Stock_data(start_time = start_time, end_time=end_time)\n",
    "\n",
    "X_binary, y_binary = feature_engineering(data,k,h,supervised= True)\n",
    "X_regression, y_regression = feature_engineering(data,k,h,supervised= True,binary= False)\n",
    "X_sharpeloss, y_sharpeloss = feature_engineering(data,k,h,supervised= False)\n",
    "\n",
    "test_data = data[data.index > datetime(pd.to_datetime(start_time).year + 5,12,31)]\n",
    "\n",
    "model_name = ['classic_TSMOM','train_decision_tree','train_xgboost','train_MLP_supervised','train_Lasso_supervised','train_MLP_supervised_reg','train_Lasso_supervised_reg','train_MLP_sharpeLoss','train_Lasso_sharpeLoss']\n",
    "\n",
    "for model in model_name:\n",
    "    \n",
    "    if model == 'classic_TSMOM':\n",
    "        func = globals()[model]\n",
    "        signal = func(test_data,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "    elif model[-3:] == 'reg':\n",
    "        func = globals()[model[:-4]]\n",
    "        X_train = np.array(X_regression[X_regression.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_regression[y_regression.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_regression[(X_regression.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_regression.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_regression[(y_regression.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_regression.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "        loss_history(history,str(model))\n",
    "        del history\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal_1.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "        \n",
    "        del temp_model\n",
    "\n",
    "    elif model in ['train_decision_tree','train_xgboost','train_MLP_supervised','train_Lasso_supervised']:\n",
    "        func = globals()[model]\n",
    "        X_train = np.array(X_binary[X_binary.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_binary[y_binary.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_binary[(X_binary.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_binary.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_binary[(y_binary.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_binary.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "\n",
    "        if model in ['train_MLP_supervised','train_Lasso_supervised']:\n",
    "            temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "            loss_history(history,str(model))\n",
    "            del history\n",
    "        else:\n",
    "            temp_model = func(X_train,y_train,X_val,y_val,k,h)\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "\n",
    "        del temp_model\n",
    "\n",
    "    else:\n",
    "        func = globals()[model]\n",
    "        X_train = np.array(X_sharpeloss[X_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "        y_train = np.array(y_sharpeloss[y_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 4,12,31)], dtype=np.float64)\n",
    "\n",
    "        X_val = np.array(X_sharpeloss[(X_sharpeloss.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (X_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        y_val = np.array(y_sharpeloss[(y_sharpeloss.index > datetime(pd.to_datetime(start_time).year + 4,12,31)) & (y_sharpeloss.index <= datetime(pd.to_datetime(start_time).year + 5,12,31))], dtype=np.float64)\n",
    "        \n",
    "        temp_model,history = func(X_train,y_train,X_val,y_val,k,h)\n",
    "        loss_history(history,str(model))\n",
    "        del history\n",
    "        signal = test_model_TSMOM(test_data,temp_model,k,h)\n",
    "        # signal_1.to_csv(\"signal_\" + str(model) + \".csv\")\n",
    "        pnl = strategy_daily_return(backtest(test_data,signal,k,h)[0])\n",
    "        \n",
    "        del temp_model\n",
    "\n",
    "    try:\n",
    "        temp = pnl.to_list()\n",
    "        temp.insert(0,model)\n",
    "        stats.loc[len(stats.index)] = temp\n",
    "    except:\n",
    "        index = pnl.index.to_list()\n",
    "        index.insert(0,'Model')\n",
    "        stats = pd.DataFrame(columns = index)\n",
    "        temp = pnl.to_list()\n",
    "        temp.insert(0,model)\n",
    "        stats.loc[len(stats.index)] = temp\n",
    "\n",
    "\n",
    "stats.to_csv(\"result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train theo Rolling Window"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
